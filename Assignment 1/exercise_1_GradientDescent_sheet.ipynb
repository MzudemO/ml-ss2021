{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Machine Learning SoSe21 Practice Class\n",
    "\n",
    "Dr. Timo Baumann, Dr. Özge Alaçcam, Björn Sygo <br>\n",
    "Email: baumann@informatik.uni-hamburg.de, alacam@informatik.uni-hamburg.de, 6sygo@informatik.uni-hamburg.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 1\n",
    "**Description:** Implement linear regression with gradient descent <br>\n",
    "**Deadline:** Saturday, 24. April 2020, 23:59 <br>\n",
    "**Working together:** You can work in pairs or triples but no larger teams are allowed. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; Please adhere to the honor code discussed in class. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; All members of the team must get involved in understanding and coding the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Submission: \n",
    "**Linus Geewe, Christoph Brauer, Moritz Lahann**\n",
    "\n",
    "*Also put high-level comments that should be read before looking at your code and results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Goal\n",
    "\n",
    "1. The goal is to find a function <br>\n",
    " $ h_{\\theta}(\\textbf{x}) = \\theta_{0} + \\theta_{1} x^{(1)}  + \\theta_{2} x^{(2)} + ... +\\theta_{D}x^{(D)} $ <br>\n",
    "that approximates the function $sin(2 \\pi)$ based on some $m$ training observations.\n",
    "\n",
    "2. Our model is a polynomial model, i.e., we use a polynomial function where $x^{j}$ is the $j$'th feature of $x$.\n",
    "\n",
    "3. You will also need to compute the root mean squared error of your model as compared to the desired outcomes in your $m$ training samples:<br>\n",
    "$E_{RMS}= \\sqrt{2E(\\theta)/m}$, where<br>\n",
    "$E(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 $ is the model's squared error over all training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generate your training samples\n",
    "\n",
    "**Task 1** (10%):  Generate $m$ artificial data points $(x_{i}; y_{i})$ where each $x_i$ is randomly generated from the interval [0,1] and\n",
    "$y_{i} = sin(2 \\pi x_{i}) + \\varepsilon $. Here, $\\varepsilon$ is a random noise value in the interval [-0.3; 0.3].\n",
    "\n",
    "Plot your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define your initial values\n",
    "\n",
    "**Task 2** (5%): Define your initial learning rate constant $\\alpha$.\n",
    "\n",
    "Also define your polynomial degree $D$ and create the initial polynomial parameters $\\theta_i$. These should be randomly generated in the interval [-0.5,0.5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Loop {<br>\n",
    "&emsp;    for $i:=1$ to $m$ { <br>\n",
    "&emsp;&emsp;        $\\theta_j := \\theta_j + \\alpha[y_i - h_{\\theta}(x_i)](x_i)_{(j)} $ (for every j)<br>\n",
    "&emsp;    }<br>\n",
    "}\n",
    "\n",
    "where:\n",
    "\n",
    "* $i$ is an index defined over the number of data points, from $i=1$ to $m$\n",
    "* $j$ is an index defined over the terms of the polynomial, from $j=0$ to $j=D$\n",
    "* The last factor $(x_i)_{(j)}$ means: the factor multiplying parameter $\\theta_j$ in the polynomial function, which in this case will be $x_i$ to the power of $j$.\n",
    "* loop for a given number of *epochs*; you may need a large number of epochs to get a good fit (e.g. 10000).\n",
    "\n",
    "**Task 3** (40%):  Implement in Python the Stochastic Gradient Descent algorithm to solve the regression problem using the datapoints you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Task 4** (15%): Implement the root mean squared error metric to measure the performance of your polynomial model wrt. your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training a model\n",
    "\n",
    "**Task 5** (15%): Train a polynomial model using your artifically created data; take note of the error of your model over the course of the training epochs. \n",
    "\n",
    "Plot (in one plot, using different colors): \n",
    "1. the data points, \n",
    "2. the original sine function, and \n",
    "3. the learned polynomial function.\n",
    "\n",
    "Plot a second graph showing the error curve. It should clearly illustrate how the error of your model decreases as the number of iterations increases.\n",
    "\n",
    "Report the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Experiment with the meta-parameters\n",
    "\n",
    "**Task 6** (15%): Try different values for $\\alpha$, $D$, $m$, and the number of epochs to investigate their effect on the learning process.\n",
    "\n",
    "Report how the model performance is influenced by changes to each of the meta-parameters and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hint\n",
    "\n",
    "A good initial value for $m$ is 100. You can also simply use a function that each time it is called generates a new point from the distribution above, i.e., you set $m$ to $\\infty$; in that case you must of course limit the number of samples chosen per iteration (how about 1?) and for evaluating the model (something >1).\n",
    "\n",
    "A good initial value for $D$ is 5.\n",
    "\n",
    "A good initial value for $\\alpha$ is 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Report Submission\n",
    "\n",
    "Prepare a report of your solution as a commented Jupyter notebook (using markdown for your results and comments); include figures and results.\n",
    "If you must, you can also upload a PDF document with the report annexed with your Python code.\n",
    "\n",
    "Upload your report file to the Machine Learning Moodle Course page. Please make sure that your submission team corresponds to the team's Moodle group that you're in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3-ubuntu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}