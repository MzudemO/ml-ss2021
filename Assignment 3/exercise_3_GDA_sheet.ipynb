{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Machine Learning SoSe21 Practice Class\n",
    "\n",
    "Dr. Timo Baumann, Dr. Özge Alaçam, Björn Sygo <br>\n",
    "Email: baumann@informatik.uni-hamburg.de, alacam@informatik.uni-hamburg.de, 6sygo@informatik.uni-hamburg.de\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 3\n",
    "**Description:** Implement gaussian discriminant analysis on the provided images <br>\n",
    "**Deadline:** Saturday, 15. May 2021, 23:59 <br>\n",
    "**Working together:** You can work in pairs or triples but no larger teams are allowed. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; Please adhere to the honor code discussed in class. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; All members of the team must get involved in understanding and coding the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Submission: \n",
    "**Christoph Brauer, Linus Geewe, Moritz Lahann**\n",
    "\n",
    "*Also put high-level comments that should be read before looking at your code and results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Goal\n",
    "\n",
    "1. The goal of this exercise is to build a classifier for real-life data (images).\n",
    "2. You will derive features of the images and then use GDA for classification, i.e., you compute the probability of each image being sampled from one of the classes $j$ (in our case $j \\in \\{\\textrm{no_mask}, \\textrm{mask}\\}$). This probability can be calculated with <br>\n",
    "$p(y=j|x)= \\frac{p(x|y=j)p(y=j)}{p(x|y=j)p(y=j)+p(x|y \\neq j)p(y \\neq j)}$ <br>\n",
    "where <br>\n",
    "$p(x|y=j)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu_j)^T \\Sigma^{-1} (x-\\mu_j)}$ <br>\n",
    "and the prior probability $p(y=j)=1-\\Phi$ (depending on the dataset).\n",
    "3. For later classification, you compute for each class the probability that the image is a sample of it and choose the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the images\n",
    "\n",
    "**Task 1** (15%): Load the images and represent them so that you can work with them.\n",
    "\n",
    "The dataset contains identically sized images of people with and without facemasks and the goal is to classify if an image contains a person wearing a facemask or not. Note that some images have 3 colors, some also have an alpha channel which you should probably ignore.\n",
    "\n",
    "When the images are loaded, they are represented as a 32x32x3 matrix. One of the 3 layers of the matrix is for the red (R) value, one for the \n",
    "green (G) value and one for the blue (B) value. The image itself is created by taking the values of each of theses 3 matrices at (i,j) to create the \n",
    "pixel of the image at spot (i,j). Each of the values can be in range of 0-255.\n",
    "\n",
    "First, you should load in the images (see zip files). There are two versions of the dataset, a small subsample which contains 40 images for each of the two classes, and the large full dataset (with imbalanced classes). There are multiple libraries for image representation. Try PIL or google around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(40, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image as im\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def load_imgs_numpy(path):\n",
    "    imgs = []\n",
    "    for index, imgpath in enumerate(glob.iglob(path + \"*\")):\n",
    "        img = im.open(imgpath)\n",
    "        imgs.append(np.array(img)[:, :, :3])\n",
    "    return np.array(imgs)    \n",
    "\n",
    "mask_imgs = load_imgs_numpy(\"subset/mask/\")\n",
    "nomask_imgs = load_imgs_numpy(\"subset/no_mask/\")\n",
    "\n",
    "print(mask_imgs.shape)\n",
    "im.fromarray(mask_imgs[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Obtain feature vectors\n",
    "\n",
    "**Task 2** (15%): Build a feature vector and represent each image by its corresponding feature vector.\n",
    "\n",
    "For Gaussian Discriminant Analysis, the images should be represented as feature vectors. A feature vector consists of different features of the image, for example mean or variance of pixel values, of color channels, number of \"blue\" pixels, etc. Be creative. The more discriminative your features, the better your classifier will perform.\n",
    "\n",
    "Your feature vector should contain at least 5 different features. (Note: your code below should also work with more or fewer features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_numpy(image):\n",
    "    return np.mean(image, (0, 1))\n",
    "\n",
    "def std_numpy(image):\n",
    "    return np.std(image, (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[188.         455.         137.10058594 124.89746094 120.07910156\n  53.05155437  54.79423721  54.67546825  99.76337891  93.5296875\n  90.86425781]\n"
     ]
    }
   ],
   "source": [
    "# maskfilter mean\n",
    "# disregards pixels outside of where we expect a mask to be\n",
    "maskFilter = [\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "[0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0],\n",
    "[0, 0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0],\n",
    "[0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "def masked_mean_numpy(image, mask):\n",
    "    mask = np.array([np.array(row) for row in mask])\n",
    "    masked_image = image * np.stack((mask, mask, mask), 2)\n",
    "\n",
    "    return mean_numpy(masked_image)\n",
    "\n",
    "blue_min = [0, 0, 100]\n",
    "blue_max = [180, 180, 255]\n",
    "\n",
    "\n",
    "white_min = [128,128,128]\n",
    "white_max = [255,255,255]\n",
    "\n",
    "def countColorValues(image, color_min, color_max):\n",
    "    count = 0\n",
    "    for row in image:\n",
    "        for pixel in row:\n",
    "            is_in_range = True\n",
    "            for index in range(len(pixel)):\n",
    "                if not color_min[index] <= pixel[index] <= color_max[index]:\n",
    "                    is_in_range = False\n",
    "            count += is_in_range\n",
    "    return count\n",
    "\n",
    "def fv_numpy(image):\n",
    "    blue_value = countColorValues(image, blue_min, blue_max)\n",
    "    white_value = countColorValues(image, white_min, white_max)\n",
    "    mean = mean_numpy(image)\n",
    "    sigma = std_numpy(image)\n",
    "    mask_filter = masked_mean_numpy(image, maskFilter)\n",
    "    return np.concatenate((np.array([blue_value, white_value]), mean, sigma, mask_filter)).flatten()\n",
    "\n",
    "print(fv_numpy(mask_imgs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Initialize your parameters\n",
    "\n",
    "**Task 3** (5%):  Initialize your parameters for the GDA algorithm.\n",
    "\n",
    "For the discriminant analysis, you will need to estimate your parameters $\\Phi, \\mu_0, \\mu_1, \\Sigma$.\n",
    "\n",
    "For this, you will need to initialize them first. You can just initalize them with 0, but you should consider their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n0\n"
     ]
    }
   ],
   "source": [
    "test_fv = fv_numpy(nomask_imgs[0])\n",
    "\n",
    "# our mean vectors have the same dimension as our feature vector (we don't initialize them separately here)\n",
    "mu_initial = np.zeros_like(test_fv)\n",
    "print(mu_initial)\n",
    "\n",
    "# our covariance matrix is a 2D matrix with dimensions n x n (where n: length of mu)\n",
    "mu_for_sigma = mu_initial.reshape(1, mu_initial.shape[0])\n",
    "sigma_initial = mu_for_sigma * mu_for_sigma.T\n",
    "print(sigma_initial)\n",
    "\n",
    "# our phi is a scalar value denoting the probability of a test sample having one of two classes \n",
    "phi = 0\n",
    "print(phi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implement Gaussian Discriminant analysis\n",
    "\n",
    "**Task 4** (35%):  Implement the Gaussian Discriminant Analysis algorithm.\n",
    "\n",
    "Now you can use the GDA algorithm to find an estimation for the correct parameters to classify the images later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.60295561e+04 -1.21211945e+04 -2.47075010e+03 -1.65095995e+03\n  -1.36891592e+03 -7.24752731e+02 -6.55641386e+02 -5.93169748e+02\n  -1.96274078e+03 -1.37345387e+03 -1.16558093e+03]\n [-1.21211945e+04  2.41673342e+04  3.80209400e+03  3.32456954e+03\n   3.08275036e+03 -1.12327598e+02 -9.80440498e+01 -1.80767328e+02\n   2.62930952e+03  2.24270160e+03  2.04797841e+03]\n [-2.47075010e+03  3.80209400e+03  7.74366219e+02  5.98267789e+02\n   5.16087192e+02 -1.25553767e+01 -7.61181978e+00 -1.05765456e+01\n   5.39302447e+02  4.06764202e+02  3.46011779e+02]\n [-1.65095995e+03  3.32456954e+03  5.98267789e+02  5.39322997e+02\n   4.91100784e+02 -5.73171187e+01 -3.79920695e+01 -3.71261862e+01\n   4.05891597e+02  3.61790645e+02  3.26271419e+02]\n [-1.36891592e+03  3.08275036e+03  5.16087192e+02  4.91100784e+02\n   4.84883669e+02 -3.83392540e+01 -2.38497445e+01 -2.71499760e+01\n   3.52806426e+02  3.32766351e+02  3.25757627e+02]\n [-7.24752731e+02 -1.12327598e+02 -1.25553767e+01 -5.73171187e+01\n  -3.83392540e+01  1.56963206e+02  1.22914077e+02  1.04303784e+02\n   3.05600417e+01 -6.79411043e+00  4.44100594e+00]\n [-6.55641386e+02 -9.80440498e+01 -7.61181978e+00 -3.79920695e+01\n  -2.38497445e+01  1.22914077e+02  1.10975179e+02  9.85700120e+01\n   3.25273438e+01  9.20524586e+00  1.66886029e+01]\n [-5.93169748e+02 -1.80767328e+02 -1.05765456e+01 -3.71261862e+01\n  -2.71499760e+01  1.04303784e+02  9.85700120e+01  9.55840437e+01\n   2.70127940e+01  9.03567137e+00  1.58814713e+01]\n [-1.96274078e+03  2.62930952e+03  5.39302447e+02  4.05891597e+02\n   3.52806426e+02  3.05600417e+01  3.25273438e+01  2.70127940e+01\n   4.01403614e+02  2.96116376e+02  2.53390678e+02]\n [-1.37345387e+03  2.24270160e+03  4.06764202e+02  3.61790645e+02\n   3.32766351e+02 -6.79411043e+00  9.20524586e+00  9.03567137e+00\n   2.96116376e+02  2.62926416e+02  2.40563438e+02]\n [-1.16558093e+03  2.04797841e+03  3.46011779e+02  3.26271419e+02\n   3.25757627e+02  4.44100594e+00  1.66886029e+01  1.58814713e+01\n   2.53390678e+02  2.40563438e+02  2.39588428e+02]]\nTrue\n"
     ]
    }
   ],
   "source": [
    "def calc_phi(mask_images, nomask_images):\n",
    "    phi = len(mask_imgs)/(len(mask_imgs)+(len(nomask_imgs)))\n",
    "    return phi\n",
    "\n",
    "def calc_mu_numpy(images):\n",
    "    fvs = []\n",
    "    for image in images:\n",
    "        fvs.append(fv_numpy(image))\n",
    "    \n",
    "    return np.mean(np.array(fvs), 0)\n",
    "\n",
    "def calc_sigma_numpy(mask_images, nomask_images, mu_mask, mu_nomask):\n",
    "    return calc_difference_numpy(mask_images, mu_mask) + calc_difference_numpy(nomask_images, mu_nomask) / (mask_imgs.shape[0] + nomask_imgs.shape[0])\n",
    "\n",
    "def calc_difference_numpy(images, mu):\n",
    "    sigmas = []\n",
    "    for image in images:\n",
    "        fv = fv_numpy(image)\n",
    "        difference_mask = np.reshape(fv - mu, (1, mu.shape[0]))\n",
    "        sigmas.append(difference_mask * difference_mask.T)\n",
    "    return np.mean(np.array(sigmas), 0)\n",
    "\n",
    "mu_mask = calc_mu_numpy(mask_imgs)\n",
    "mu_nomask = calc_mu_numpy(nomask_imgs)\n",
    "sigma = calc_sigma_numpy(mask_imgs, nomask_imgs, mu_mask, mu_nomask)\n",
    "print(sigma)\n",
    "# confirm that sigma is symmetric\n",
    "print(np.allclose(sigma, sigma.T, rtol=1e-05, atol=1e-08))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Classify with the Bayes rule\n",
    "**Task 5** (10%): Use the Bayes rule to check how many images were correctly classified.\n",
    "\n",
    "Now that you have estimated your parameters, you can use the Bayes rule to classify the images. You then can evaluate how many of the images were correctly classified and try again with different features if the results weren't good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multivariate Gaussian PDF\n",
    "def pdf(x, mu, sigma):\n",
    "    d = mu.shape[0]\n",
    "    first_term = 1 / (((2 * math.pi) ** (d / 2)) * (np.linalg.det(sigma) ** 0.5))\n",
    "    difference_vector = x - mu\n",
    "    exponent = -0.5 * (np.linalg.inv(sigma).dot(difference_vector.T).dot(difference_vector))\n",
    "    second_term = math.exp(exponent)\n",
    "    return first_term * second_term\n",
    "\n",
    "# BAYES RULE\n",
    "# p(y = 1|x) = (p(x|y = 1) * p(y = 1) / p(x))\n",
    "# where:    p(y = 1) is our prior phi\n",
    "#           p(x|y = 1) is pdf(x, mu for y = 1, sigma)\n",
    "#           p(x) is constant and equal for both classes, so we can disregard it (since we don't care about the actual probability)\n",
    "\n",
    "# returns 1 if mask, 0 if no mask\n",
    "def bayes_rule(x, mu_mask, mu_nomask, sigma, phi):\n",
    "    prob_mask = pdf(x, mu_mask, sigma) * phi\n",
    "    prob_nomask = pdf(x, mu_nomask, sigma) * phi\n",
    "    return prob_mask > prob_nomask\n",
    "\n",
    "# expects samples to be in format [[[...image...], class], ...]\n",
    "def accuracy(samples, mu_mask, mu_nomask, sigma, phi):\n",
    "    correct = 0\n",
    "    for sample in samples:\n",
    "        fv = fv_numpy(sample[0])\n",
    "        correct += bayes_rule(fv, mu_mask, mu_nomask, sigma, phi) == sample[1]\n",
    "    return correct, correct / samples.shape[0]\n",
    "    \n",
    "# puts all samples into one list next to their class labels\n",
    "def prepare_samples(mask_images, nomask_images):\n",
    "    samples = []\n",
    "    for image in mask_images:\n",
    "        samples.append(np.array([image, 1], dtype=object))\n",
    "    for image in nomask_images:\n",
    "        samples.append(np.array([image, 0], dtype=object))\n",
    "    return np.array(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correctly Predicted: 77/80\nAccuracy: 96.25%\n"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "mask_imgs = load_imgs_numpy(\"subset/mask/\")\n",
    "nomask_imgs = load_imgs_numpy(\"subset/no_mask/\")\n",
    "\n",
    "# Get ML params\n",
    "mu_mask = calc_mu_numpy(mask_imgs)\n",
    "mu_nomask = calc_mu_numpy(nomask_imgs)\n",
    "sigma = calc_sigma_numpy(mask_imgs, nomask_imgs, mu_mask, mu_nomask)\n",
    "phi = calc_phi(mask_imgs, nomask_imgs)\n",
    "\n",
    "# Predict\n",
    "concat_samples = prepare_samples(mask_imgs, nomask_imgs)\n",
    "nr_correct, acc = accuracy(concat_samples, mu_mask, mu_nomask, sigma, phi)\n",
    "print(f\"Correctly Predicted: {nr_correct}/{concat_samples.shape[0]}\")\n",
    "print(f\"Accuracy: {acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cross-validation\n",
    "\n",
    "**Task 6** (10%): Implement 10-fold cross-validation and report the quality of your results in terms of accuracy and f-score.\n",
    "\n",
    "You have so far trained and tested your classifier on the same data. This does not tell us much about the true performance on unseen data. \n",
    "Instead, you should now randomly split your data into _k_ folds of equal size. you then train your model _k_ times, using all but the _k_'th fold for training and the _k_'th fold for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(samples, k):\n",
    "    counts = []\n",
    "    accs = []\n",
    "    individual_folds = np.array_split(samples, k)\n",
    "    for index, fold in enumerate(individual_folds):\n",
    "        # remove validation fold from training and concatenate training folds to single array\n",
    "        train = np.delete(individual_folds, index, 0)\n",
    "        train = np.concatenate(train, 0)\n",
    "        val = fold\n",
    "\n",
    "        # split into mask and non mask images\n",
    "        mask_imgs = np.array([sample[0] for sample in train if sample[1] == 1])\n",
    "        nomask_imgs = np.array([sample[0] for sample in train if sample[1] == 0])\n",
    "\n",
    "        # calculate mu, sigma, ... for train\n",
    "        mu_mask = calc_mu_numpy(mask_imgs)\n",
    "        mu_nomask = calc_mu_numpy(nomask_imgs)\n",
    "        sigma = calc_sigma_numpy(mask_imgs, nomask_imgs, mu_mask, mu_nomask)\n",
    "        phi = calc_phi(mask_imgs, nomask_imgs)\n",
    "\n",
    "        # test on validation fold\n",
    "        results = accuracy(val, mu_mask, mu_nomask, sigma, phi)\n",
    "        counts.append(results[0])\n",
    "        accs.append(results[1])\n",
    "        \n",
    "    mean_count = np.mean(np.array(counts))\n",
    "    total_count = sum(counts)\n",
    "    mean_acc = np.mean(np.array(accs))\n",
    "    return mean_count, mean_acc, total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(80, 2)\n",
      "Total Correctly Predicted: 77/80\n",
      "Avg. Correctly Predicted: 7.7/8.0\n",
      "Accuracy: 96.25%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4505918)\n",
    "\n",
    "shuffled_samples = prepare_samples(mask_imgs, nomask_imgs)\n",
    "np.random.shuffle(shuffled_samples)\n",
    "print(shuffled_samples.shape)\n",
    "# im.fromarray(shuffled_samples[0, 0]).show()\n",
    "\n",
    "k = 10\n",
    "mean_count, mean_acc, total_count = k_fold_cross_validation(shuffled_samples, k)\n",
    "print(f\"Total Correctly Predicted: {total_count}/{shuffled_samples.shape[0]}\")\n",
    "print(f\"Avg. Correctly Predicted: {mean_count}/{shuffled_samples.shape[0] / k}\")\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "source": [
    "There is some variance here due to randomness from the shuffling, though not much - one less correct sometimes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature importance\n",
    "\n",
    "**Task 7** (10%): Experiment with the features: how well does the classifier perform with individual features, what is the additional value of the second best feature in addition to the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make feature vector changeable\n",
    "class GDA:\n",
    "    def __init__(self, feature_vector_func):\n",
    "        self.fv = feature_vector_func\n",
    "\n",
    "    def pdf(self, x, mu, sigma):\n",
    "        d = mu.shape[0]\n",
    "        first_term = 1 / (((2 * math.pi) ** (d / 2)) * (np.linalg.det(sigma) ** 0.5))\n",
    "        difference_vector = x - mu\n",
    "        exponent = -0.5 * (np.linalg.inv(sigma).dot(difference_vector.T).dot(difference_vector))\n",
    "        second_term = math.exp(exponent)\n",
    "        return first_term * second_term\n",
    "\n",
    "    def bayes_rule(self, x, mu_mask, mu_nomask, sigma, phi):\n",
    "        prob_mask = self.pdf(x, mu_mask, sigma) * phi\n",
    "        prob_nomask = self.pdf(x, mu_nomask, sigma) * phi\n",
    "        return prob_mask > prob_nomask\n",
    "\n",
    "    def accuracy(self, samples, mu_mask, mu_nomask, sigma, phi):\n",
    "        correct = 0\n",
    "        for sample in samples:\n",
    "            fv = self.fv(sample[0])\n",
    "            correct += self.bayes_rule(fv, mu_mask, mu_nomask, sigma, phi) == sample[1]\n",
    "        return correct, correct / samples.shape[0]\n",
    "\n",
    "    def calc_phi(self, mask_images, nomask_images):\n",
    "        phi = len(mask_imgs)/(len(mask_imgs)+(len(nomask_imgs)))\n",
    "        return phi\n",
    "\n",
    "    def calc_mu_numpy(self, images):\n",
    "        fvs = []\n",
    "        for image in images:\n",
    "            fvs.append(self.fv(image))\n",
    "        \n",
    "        return np.mean(np.array(fvs), 0)\n",
    "\n",
    "    def calc_sigma_numpy(self, mask_images, nomask_images, mu_mask, mu_nomask):\n",
    "        return self.calc_difference_numpy(mask_images, mu_mask) + self.calc_difference_numpy(nomask_images, mu_nomask) / (mask_imgs.shape[0] + nomask_imgs.shape[0])\n",
    "\n",
    "    def calc_difference_numpy(self, images, mu):\n",
    "        sigmas = []\n",
    "        for image in images:\n",
    "            fv = self.fv(image)\n",
    "            difference_mask = np.reshape(fv - mu, (1, mu.shape[0]))\n",
    "            sigmas.append(difference_mask * difference_mask.T)\n",
    "        return np.mean(np.array(sigmas), 0)\n",
    "\n",
    "    def k_fold_cross_validation(self, samples, k):\n",
    "        counts = []\n",
    "        accs = []\n",
    "        individual_folds = np.array_split(samples, k)\n",
    "        for index, fold in enumerate(individual_folds):\n",
    "            train = np.delete(individual_folds, index, 0)\n",
    "            train = np.concatenate(train, 0)\n",
    "            val = fold\n",
    "\n",
    "            mask_imgs = np.array([sample[0] for sample in train if sample[1] == 1])\n",
    "            nomask_imgs = np.array([sample[0] for sample in train if sample[1] == 0])\n",
    "\n",
    "            mu_mask = self.calc_mu_numpy(mask_imgs)\n",
    "            mu_nomask = self.calc_mu_numpy(nomask_imgs)\n",
    "            sigma = self.calc_sigma_numpy(mask_imgs, nomask_imgs, mu_mask, mu_nomask)\n",
    "            phi = self.calc_phi(mask_imgs, nomask_imgs)\n",
    "\n",
    "            results = self.accuracy(val, mu_mask, mu_nomask, sigma, phi)\n",
    "            counts.append(results[0])\n",
    "            accs.append(results[1])\n",
    "            \n",
    "        mean_count = np.mean(np.array(counts))\n",
    "        total_count = sum(counts)\n",
    "        mean_acc = np.mean(np.array(accs))\n",
    "        return mean_count, mean_acc, total_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7.6, 0.95, 76)\n",
      "(7.0, 0.875, 70)\n",
      "(4.5, 0.5625, 45)\n",
      "(6.5, 0.8125, 65)\n",
      "(7.9, 0.9875, 79)\n"
     ]
    }
   ],
   "source": [
    "# overwrite feature vector method with different single features\n",
    "# call k_fold_cross_validation for each feature\n",
    "# compare\n",
    "# overwrite fv with best & second best feature, validate, compare to only single best\n",
    "\n",
    "# shuffle samples once\n",
    "shuffled_samples = prepare_samples(mask_imgs, nomask_imgs)\n",
    "np.random.shuffle(shuffled_samples)\n",
    "k = 10\n",
    "\n",
    "# per channel mean\n",
    "def mean_fv(image):\n",
    "    return mean_numpy(image)\n",
    "\n",
    "# per channel stddev\n",
    "def std_fv(image):\n",
    "    return std_numpy(image)\n",
    "\n",
    "# nr of \"blue\" pixels in image\n",
    "blue_min = [0, 0, 100]\n",
    "blue_max = [180, 180, 255]\n",
    "def blue_fv(image):\n",
    "    return np.array([countColorValues(image, blue_min, blue_max)])\n",
    "\n",
    "# nr of \"white\" pixels in image\n",
    "white_min = [128,128,128]\n",
    "white_max = [255,255,255]\n",
    "def white_fv(image):\n",
    "    return np.array([countColorValues(image, white_min, white_max)])\n",
    "\n",
    "# per channel mean of masked image\n",
    "def masked_fv(image):\n",
    "    return masked_mean_numpy(image, maskFilter)\n",
    "\n",
    "print(GDA(mean_fv).k_fold_cross_validation(shuffled_samples, k))\n",
    "\n",
    "print(GDA(std_fv).k_fold_cross_validation(shuffled_samples, k))\n",
    "\n",
    "print(GDA(blue_fv).k_fold_cross_validation(shuffled_samples, k))\n",
    "\n",
    "print(GDA(white_fv).k_fold_cross_validation(shuffled_samples, k))\n",
    "\n",
    "print(GDA(masked_fv).k_fold_cross_validation(shuffled_samples, k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8.0, 1.0, 80)\n"
     ]
    }
   ],
   "source": [
    "def top_two_fv(image):\n",
    "    mean = mean_numpy(image)\n",
    "    masked_mean = masked_mean_numpy(image, maskFilter)\n",
    "    return np.concatenate([mean, masked_mean])\n",
    "\n",
    "print(GDA(top_two_fv).k_fold_cross_validation(shuffled_samples, k))"
   ]
  },
  {
   "source": [
    "# Feature importance\n",
    "\n",
    "Our top two features were \n",
    " - mean per channel on masked image with 98.75% accuracy\n",
    " - mean per channel with 96.25% accuracy \n",
    "\n",
    "The mask used to mask the image disregarded pixels outside of the lower center of the image (where the mask was commonly placed in the dataset), so there was a lower influence of backgrounds on the per-channel means (e.g. some images had white backgrounds).\n",
    "\n",
    "Blue pixels performed the worst, probably because a larger number of images with face masks were actually of white face masks.\n",
    "\n",
    "Suprisingly, even just the channel means alone give quite a respectable result (on a simple dataset like this).\n",
    "\n",
    "Using the top two features increased accuracy from 98.75% to 100%. This means some features in our full feature vector used above were actually detrimental to performance. Features are not weighted, so lower confidence features have the same impact as high confidence ones, worsening performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Report Submission\n",
    "\n",
    "Prepare a report of your solution as a commented Jupyter notebook (using markdown for your results and comments); include figures and results.\n",
    "If you must, you can also upload a PDF document with the report annexed with your Python code.\n",
    "\n",
    "Upload your report file to the Machine Learning Moodle Course page. Please make sure that your submission team corresponds to the team's Moodle group that you're in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0b26a19bd2e80a295d93b9c279d2e5c53e7eab4d92e561a7637ff5053bd657605",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}