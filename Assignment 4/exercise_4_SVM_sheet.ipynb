{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Machine Learning SoSe21 Practice Class\n",
    "\n",
    "Dr. Timo Baumann, Dr. Özge Alaçam, Björn Sygo <br>\n",
    "Email: baumann@informatik.uni-hamburg.de, alacam@informatik.uni-hamburg.de, 6sygo@informatik.uni-hamburg.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 4\n",
    "**Description:** Analyse and work with a machine learning library for support vector machines <br>\n",
    "**Deadline:** Saturday, 22.05.2021, 23:59 <br>\n",
    "**Working together:** You can work in pairs or triples but no larger teams are allowed. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; Please adhere to the honor code discussed in class. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; All members of the team must get involved in understanding and coding the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Submission: \n",
    "**Christoph Brauer, Linus Geewe, Moritz Lahann**\n",
    "\n",
    "*Also put high-level comments that should be read before looking at your code and results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Goal\n",
    " 1. analyze existing code of machine learning algorithms (in this case for training SVMs)\n",
    " 2. experiment with integrating and using code that is available in libraries.\n",
    "\n",
    "You will be working with two libraries, one implemented in C++ (LibSVM, see https://github.com/cjlin1/libsvm) and the other in JAVA (part of the WEKA library, see https://github.com/Waikato/weka-3.8).\n",
    "\n",
    "Note that if you are running Windows, doing the C++ parts of this exercise in the Windows Subsystem for Linux may (or may not) simplify your life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1: Mapping Code to Machine Learning Concepts\n",
    "\n",
    "In the following, we ask you to relate concepts used in SVMs to the code in the two implementations. When we ask you to identify where in the code a given concept is given, please report the filename(s) and line(s) as well as the class/function/variable names.\n",
    "\n",
    "\n",
    "### Kernels\n",
    "1. Find the code that defines the abstract kernel definition and report what types of kernels are available in WEKA/LibSVM respectively (as well as where/how they are implemented).\n",
    "2. How would you go about implementing an additional kernel?\n",
    "3. Describe the process involved in computing the dot product (`svm.cpp:294ff.`, operation `Kernel::dot` for LibSVM; `weka.classifiers.functions.supportVector.CachedKernel#dotProd`, lines 292ff. for WEKA) including what are the arguments to the respective functions.\n",
    "4. Describe the caching that is going on in both libraries (LRU refers to least-recently-used cache). What is being cached and why is this relevant?\n",
    "\n",
    "### Sequential Minimal Optimization\n",
    "\n",
    "1. As described in the lectures, SMO alternates between finding two parameters α to optimize, and optimizing their values. Find this loop in both implementations and name the methods used for performing the two steps.\n",
    "2. Identify the code that performs the 'clipping' of lagrange multipliers as described in the lecture notes on page 25. Can you find the optimization computations involved in finding the new value for $α_i$ and $α_j$?\n",
    "3. Describe where in the code the mapping function $ϕ$ is being computed.\n",
    "\n",
    "### Hints\n",
    "* Download both WEKA and LibSVM from the URLs given above (preferably use `git clone`).\n",
    "* For LibSVM, you'll primarily look at `svm.cpp`. Any editor with syntax highlighting will do.\n",
    "* Open the WEKA code in the IDE of your choice (IntelliJ, Eclipse, ...). WEKA comes with an Eclipse project definition that IntelliJ can import as well.\n",
    "* Sometimes it may be easier to start the analysis with LibSVM, sometimes with WEKA -- although C++ may be less familiar, it's easy to get lost in the WEKA class hierarchies; conversely, the class hierarchy may also help understanding."
   ]
  },
  {
   "source": [
    "### Kernels\n",
    "\n",
    "1. \n",
    " * libsvm  \n",
    " The abstract kernel is defined as a class (svm.cpp:202). Specific kernels are implemented by deciding which kernel function to use in the constructor of the Kernel class (svm.cpp:253). The available kernels are LINEAR, POLY, RBF, SIGMOID, and PRECOMPUTED.\n",
    "\n",
    " * Weka  \n",
    " The abstract kernel class is implemented in `supportVector\\Kernel.java`. The kernel implementations are classes in other files (e.g. `supportVector\\RBFKernel.java`) that extend the Kernel class and overwrite its methods as needed, e.g. `buildKernel` and `evaluate`. The available kernels are Poly, NormalizedPoly, RBF, Subsequence (`StringKernel.java`), PrecomputedKernel, and Pearson universal kernel (`PUK.java`).\n",
    "\n",
    "2.\n",
    " * libsvm  \n",
    " Define a new kernel function in the Kernel class (svm.cpp:250)  \n",
    " Add the new function to the kernel type switch case (svm.cpp:273)  \n",
    " * Weka  \n",
    " Create a new class in the supportVector folder which extends Kernel or CachedKernel.\n",
    "\n",
    "3.\n",
    " * libsvm  \n",
    " The dot product takes two pointers as arguments. These pointers signify the start of the two vectors. The vectors are expected to be terminated with a `svm_node` with index -1. As long as the indices are equal, the product of the values of the vectors as this position is added to the total sum. The sum is then returned.\n",
    "\n",
    " * weka   \n",
    " The dot product takes two Instances as arguments. These are a data type that store indices and corresponding values. The implementation follows the same principle as that of libsvm, values are multiplied and summed as long as indices of the two vectors are equal.\n",
    "\n",
    " 4.\n",
    "\n",
    "  * libsvm\n",
    "  The LRU-cache stores the sub-problems of the optimization problem, which means when sub-problems repeat the result can be quickly and efficiently read from the cache (`svm.cpp:1273,1283`).\n",
    "\n",
    "  * weka  \n",
    "  The implementation caches the evaluation results of the Kernel function. This means that results for repeating values can be read from cache and do not need to be recalculated from scratch, saving time.\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SMO\n",
    "\n",
    "1. \n",
    "\n",
    " * libsvm  \n",
    " The optimization loop happens at `svm.cpp:564`. `select_working_set()` is used to find the two alphas i and j to optimize. They are then updated at `svm.cpp:604` or `svm.cpp:647`.\n",
    "\n",
    " * weka  \n",
    " The optimization loop is located at `SMO.java:591`. This is called from the training loop `SMO.java:427` with `smo.buildClassifier`, taking a number of training examples as the argument. `examineExample` is called for an instance, which then performs one optimization step by calling `takeStep`. \n",
    "\n",
    "2.\n",
    "\n",
    " * libsvm  \n",
    " The clipping is done in the `update_alpha_status()` method at `svm.cpp:430`. The optimization computations are located as outlined in 1.\n",
    "\n",
    " * weka  \n",
    " The clipping happens at `SMO.java:968` or `SMO.java:987` depending on the sign of the second derivative. $a_1$ is updated to be more optimized at `SMO.java:1007` with the value of $a_2$, which is optimized at `SMO.java:965` or `SMO.java:976-986` (and then clipped). The values are saved for each step at `SMO.java:1104-1105`.\n",
    "\n",
    " 3.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2: Working with LibSVM\n",
    "\n",
    "In the second part, you will use the previously analysed code on the examples from the previous face classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup\n",
    "\n",
    "First, you will have to make the LibSVM library available to Python.\n",
    "\n",
    "You can use a pre-built python version for this; instructions are at\n",
    "\n",
    "https://pypi.org/project/libsvm/\n",
    "\n",
    "Alternatively, you can compile it from the github project you already analyzed for the first part. Further instructions are in the `README`.\n",
    "\n",
    "To use the library, you should read the documentation in the github project's `README` to inform you how to use it. You can also view the example to get a grasp on how it works in python. It shows you the neccessary functions, but to adapt them to the tasks below, you may have to look up the parameters in the documentation.\n",
    "\n",
    "Note: we are aware that the instructions above are sketchy. Get used to it and ask questions on the exercise forum. Once you're a machine learning practitioner, you'll get worse instructions on more badly maintained software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Testing different kernels for face classification\n",
    "\n",
    "Now you have to use the feature vectors you have generated in the previous exercise submission on GDA and apply the SVM algorithm of the library to classify them. You may have to convert them to the neccessary format in a text file first to utilize the library. Try testing with different kernels, namely the linear, polynomial (degrees d = 1,2,3,4), sigmoid and radial basis kernel.\n",
    "\n",
    "In this submission, please also report results on the full dataset, not only the small (balanced) 80-image set.\n",
    "\n",
    "Note that you may of course use the GDA sample solution if you are unsure about your own feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GDA sample solution\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "\n",
    "def load_imgs_numpy(path):\n",
    "    imgs = []\n",
    "    for index, imgpath in enumerate(glob.iglob(path + \"*\")):\n",
    "        img = im.open(imgpath)\n",
    "        imgs.append(np.array(img)[:, :, :3])\n",
    "    return np.array(imgs)    \n",
    "\n",
    "def prepare_samples(mask_images, nomask_images):\n",
    "    samples = []\n",
    "    for image in mask_images:\n",
    "        samples.append(np.array([image, 1], dtype=object))\n",
    "    for image in nomask_images:\n",
    "        samples.append(np.array([image, -1], dtype=object))\n",
    "    return np.array(samples)\n",
    "\n",
    "def mean_numpy(image):\n",
    "    return np.mean(image, (0, 1))\n",
    "\n",
    "def std_numpy(image):\n",
    "    return np.std(image, (0, 1))\n",
    "\n",
    "maskFilter = [\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "[0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0],\n",
    "[0, 0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0],\n",
    "[0, 0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0],\n",
    "[0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "def masked_mean_numpy(image, mask):\n",
    "    mask = np.array([np.array(row) for row in mask])\n",
    "    masked_image = image * np.stack((mask, mask, mask), 2)\n",
    "\n",
    "    return mean_numpy(masked_image)\n",
    "\n",
    "def feature_vector(image):\n",
    "    mean = mean_numpy(image)\n",
    "    masked_mean = masked_mean_numpy(image, maskFilter)\n",
    "    std = std_numpy(image)\n",
    "    return np.concatenate([mean, masked_mean, std])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1221, 2)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "mask_imgs = load_imgs_numpy(\"cutouts/with_mask/\")\n",
    "nomask_imgs = load_imgs_numpy(\"cutouts/without_mask/\")\n",
    "\n",
    "all_samples = prepare_samples(mask_imgs, nomask_imgs)\n",
    "np.random.shuffle(all_samples)\n",
    "\n",
    "print(all_samples.shape)\n",
    "\n",
    "# save to file\n",
    "with open('data_for_svm', 'w') as f:\n",
    "    for sample in all_samples:\n",
    "        target = sample[1]\n",
    "        if target > 0:\n",
    "            target = f\"+{target}\"\n",
    "        else:\n",
    "            target = f\"{target}\"\n",
    "\n",
    "        features = feature_vector(sample[0])\n",
    "        features = [f\"{index + 1}:{feature}\" for index, feature in enumerate(features)]\n",
    "\n",
    "        line = \" \".join([target, *features])\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 86.8852% (212/244) (classification)\n"
     ]
    }
   ],
   "source": [
    "from libsvm.svmutil import *\n",
    "\n",
    "y, x = svm_read_problem('./data_for_svm')\n",
    "\n",
    "split_index = round(len(y) * 0.8)\n",
    "\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "x_train = x[:split_index]\n",
    "x_test = x[split_index:]\n",
    "\n",
    "m = svm_train(y_train, x_train, '-c 4')\n",
    "\n",
    "p_label, p_acc, p_val = svm_predict(y_test, x_test, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kernel type: linear\n",
      "Accuracy = 94.2623% (230/244) (classification)\n",
      "(94.26229508196722, 0.22950819672131148, 0.5606303881517001)\n",
      "Kernel type: polynomial\n",
      "Degrees: 1\n",
      "Accuracy = 94.6721% (231/244) (classification)\n",
      "(94.67213114754098, 0.21311475409836064, 0.5841311641462237)\n",
      "Kernel type: polynomial\n",
      "Degrees: 2\n",
      "Accuracy = 95.082% (232/244) (classification)\n",
      "(95.08196721311475, 0.19672131147540983, 0.6327425826326029)\n",
      "Kernel type: polynomial\n",
      "Degrees: 3\n",
      "Accuracy = 93.0328% (227/244) (classification)\n",
      "(93.0327868852459, 0.2786885245901639, 0.49844262852347604)\n",
      "Kernel type: polynomial\n",
      "Degrees: 4\n",
      "Accuracy = 92.623% (226/244) (classification)\n",
      "(92.62295081967213, 0.29508196721311475, 0.5179958866357489)\n",
      "Kernel type: polynomial\n",
      "Accuracy = 93.0328% (227/244) (classification)\n",
      "(93.0327868852459, 0.2786885245901639, 0.49844262852347604)\n",
      "Kernel type: radial\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Kernel type: sigmoid\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n"
     ]
    }
   ],
   "source": [
    "kernel_types = [\n",
    "    {\"nr\": 0, \"name\": \"linear\"},\n",
    "    {\"nr\": 1, \"name\": \"polynomial\"},\n",
    "    {\"nr\": 2, \"name\": \"radial\"},\n",
    "    {\"nr\": 3, \"name\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "for kernel in kernel_types:\n",
    "    nr = kernel[\"nr\"]\n",
    "    name = kernel[\"name\"]\n",
    "    if kernel[\"name\"] == \"polynomial\":\n",
    "        for degree in [1, 2, 3, 4]:\n",
    "            \n",
    "            arg = f\"-t {nr} -d {degree}\"\n",
    "            m = svm_train(y_train, x_train, arg)\n",
    "\n",
    "            print(f\"Kernel type: {name}\")\n",
    "            print(f\"Degrees: {degree}\")\n",
    "            p_label, p_acc, p_val = svm_predict(y_test, x_test, m)\n",
    "            print(p_acc)\n",
    "\n",
    "    arg = f\"-t {nr}\"\n",
    "    m = svm_train(y_train, x_train, arg)\n",
    "\n",
    "    print(f\"Kernel type: {name}\")\n",
    "    p_label, p_acc, p_val = svm_predict(y_test, x_test, m)\n",
    "    print(p_acc)"
   ]
  },
  {
   "source": [
    "## Kernel types\n",
    "\n",
    "Best results are obtained with polynomial kernel (2 degrees) at 95.082%, however linear kernel and poly kernel with degree 1 are within two correctly classified samples of this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tune your parameters\n",
    "\n",
    "To effectively use kernels on your task, you will have to tune the parameters $C$ and $gamma$ for your model. Try different values to improve your results. You need not search for the overall best parameters but describe and apply some systematic approach that you use."
   ]
  },
  {
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "\n",
    "We are going for a gridsearch approach, which means looking at all possible combinations of the different values for $C$ and $gamma$ that we want to test. As for selecting the different values to test, we opt for a coarse search over different orders of magnetude. The idea behind this is that we want to get a rough idea of what the parameters might optimally be. Performance differences within a single order of magnetude (i.e. 0.2 vs 0.5) are typically small and subject to some randomness possibly. "
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "import itertools\n",
    "\n",
    "# default is 1 so we cover a range of large and small orders of magnetude\n",
    "c_values = [100, 10, 1, 0.1, 0.001, 0.0001]\n",
    "\n",
    "# default is 1/number of samples (in our case 1221) so it seems absurd to go above 1\n",
    "gamma_values = [1, 0.1, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "# 6x6 = 36 different tests\n",
    "combinations = list(itertools.product(c_values, gamma_values))\n",
    "\n",
    "accs = []\n",
    "\n",
    "for values in combinations:\n",
    "    c = values[0]\n",
    "    gamma = values[1]\n",
    "\n",
    "    # use best results from previous test\n",
    "    arg = f\"-t 1 -d 2 -c {c} -g {gamma}\"\n",
    "    m = svm_train(y_train, x_train, arg)\n",
    "\n",
    "    print(f\"Cost: {c} Gamma: {gamma}\")\n",
    "    p_label, p_acc, p_val = svm_predict(y_test, x_test, m)\n",
    "    print(p_acc)\n",
    "\n",
    "    accs.append(p_acc[0])\n",
    "\n",
    "print(f\"Maximum accuracy: {max(accs)}\")\n",
    "max_values = combinations[np.argmax(np.array(accs))]\n",
    "print(f\"Best combination: C {max_values[0]}, gamma {max_values[1]}\")"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cost: 100 Gamma: 1\n",
      "Accuracy = 94.2623% (230/244) (classification)\n",
      "(94.26229508196722, 0.22950819672131148, 0.590353405313224)\n",
      "Cost: 100 Gamma: 0.1\n",
      "Accuracy = 94.6721% (231/244) (classification)\n",
      "(94.67213114754098, 0.21311475409836064, 0.5919713092654164)\n",
      "Cost: 100 Gamma: 0.001\n",
      "Accuracy = 97.9508% (239/244) (classification)\n",
      "(97.95081967213115, 0.08196721311475409, 0.8306561095168803)\n",
      "Cost: 100 Gamma: 0.0001\n",
      "Accuracy = 97.541% (238/244) (classification)\n",
      "(97.54098360655738, 0.09836065573770492, 0.7968996854036183)\n",
      "Cost: 100 Gamma: 1e-05\n",
      "Accuracy = 91.3934% (223/244) (classification)\n",
      "(91.39344262295081, 0.3442622950819672, 0.352512631702636)\n",
      "Cost: 100 Gamma: 1e-06\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 10 Gamma: 1\n",
      "Accuracy = 94.2623% (230/244) (classification)\n",
      "(94.26229508196722, 0.22950819672131148, 0.590353405313224)\n",
      "Cost: 10 Gamma: 0.1\n",
      "Accuracy = 94.2623% (230/244) (classification)\n",
      "(94.26229508196722, 0.22950819672131148, 0.5606303881517001)\n",
      "Cost: 10 Gamma: 0.001\n",
      "Accuracy = 96.7213% (236/244) (classification)\n",
      "(96.72131147540983, 0.13114754098360656, 0.7339476098420223)\n",
      "Cost: 10 Gamma: 0.0001\n",
      "Accuracy = 95.082% (232/244) (classification)\n",
      "(95.08196721311475, 0.19672131147540983, 0.6236984273927003)\n",
      "Cost: 10 Gamma: 1e-05\n",
      "Accuracy = 88.1148% (215/244) (classification)\n",
      "(88.11475409836066, 0.47540983606557374, 0.15421095717199396)\n",
      "Cost: 10 Gamma: 1e-06\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 1 Gamma: 1\n",
      "Accuracy = 94.2623% (230/244) (classification)\n",
      "(94.26229508196722, 0.22950819672131148, 0.5606303881517001)\n",
      "Cost: 1 Gamma: 0.1\n",
      "Accuracy = 95.082% (232/244) (classification)\n",
      "(95.08196721311475, 0.19672131147540983, 0.609256534222994)\n",
      "Cost: 1 Gamma: 0.001\n",
      "Accuracy = 97.541% (238/244) (classification)\n",
      "(97.54098360655738, 0.09836065573770492, 0.7968996854036183)\n",
      "Cost: 1 Gamma: 0.0001\n",
      "Accuracy = 91.3934% (223/244) (classification)\n",
      "(91.39344262295081, 0.3442622950819672, 0.352512631702636)\n",
      "Cost: 1 Gamma: 1e-05\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 1 Gamma: 1e-06\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.1 Gamma: 1\n",
      "Accuracy = 95.9016% (234/244) (classification)\n",
      "(95.90163934426229, 0.16393442622950818, 0.6802755188072842)\n",
      "Cost: 0.1 Gamma: 0.1\n",
      "Accuracy = 96.7213% (236/244) (classification)\n",
      "(96.72131147540983, 0.13114754098360656, 0.7339476098420223)\n",
      "Cost: 0.1 Gamma: 0.001\n",
      "Accuracy = 95.082% (232/244) (classification)\n",
      "(95.08196721311475, 0.19672131147540983, 0.6236984273927003)\n",
      "Cost: 0.1 Gamma: 0.0001\n",
      "Accuracy = 88.1148% (215/244) (classification)\n",
      "(88.11475409836066, 0.47540983606557374, 0.15421095717199396)\n",
      "Cost: 0.1 Gamma: 1e-05\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.1 Gamma: 1e-06\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.001 Gamma: 1\n",
      "Accuracy = 95.9016% (234/244) (classification)\n",
      "(95.90163934426229, 0.16393442622950818, 0.6802755188072842)\n",
      "Cost: 0.001 Gamma: 0.1\n",
      "Accuracy = 96.7213% (236/244) (classification)\n",
      "(96.72131147540983, 0.13114754098360656, 0.7339476098420223)\n",
      "Cost: 0.001 Gamma: 0.001\n",
      "Accuracy = 88.1148% (215/244) (classification)\n",
      "(88.11475409836066, 0.47540983606557374, 0.15421095717199396)\n",
      "Cost: 0.001 Gamma: 0.0001\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.001 Gamma: 1e-05\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.001 Gamma: 1e-06\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.0001 Gamma: 1\n",
      "Accuracy = 97.541% (238/244) (classification)\n",
      "(97.54098360655738, 0.09836065573770492, 0.8007975064604111)\n",
      "Cost: 0.0001 Gamma: 0.1\n",
      "Accuracy = 97.541% (238/244) (classification)\n",
      "(97.54098360655738, 0.09836065573770492, 0.7968996854036183)\n",
      "Cost: 0.0001 Gamma: 0.001\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.0001 Gamma: 0.0001\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.0001 Gamma: 1e-05\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Cost: 0.0001 Gamma: 1e-06\n",
      "Accuracy = 86.4754% (211/244) (classification)\n",
      "(86.47540983606558, 0.5409836065573771, nan)\n",
      "Maximum accuracy: 97.95081967213115\n",
      "Best combination: C 100, gamma 0.001\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Results\n",
    "\n",
    "Maximum accuracy: 97.95081967213115\n",
    "Best combination: C 100, gamma 0.001\n",
    "\n",
    "We could now use that information to do a finer grid-search, for example C between 50 and 500, and gamma between 0.005 and 0.0005.\n",
    "\n",
    "We might also test out these parameter combination with different kernel types if we had lots of time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Report Submission\n",
    "\n",
    "Prepare a report of your solution as a commented Jupyter notebook (using markdown for your results and comments); include figures and results.\n",
    "If you must, you can also upload a PDF document with the report annexed with your Python code.\n",
    "\n",
    "Upload your report file to the Machine Learning Moodle Course page. Please make sure that your submission team corresponds to the team's Moodle group that you're in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0936909ed86daa26a0ac8b1eb938f5d4b7bf4e1ec07caee18ace3af08811b1082",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}