{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Machine Learning SoSe21 Practice Class\n",
    "\n",
    "Dr. Timo Baumann, Dr. Özge Alaçam, Björn Sygo <br>\n",
    "Email: baumann@informatik.uni-hamburg.de, alacam@informatik.uni-hamburg.de, 6sygo@informatik.uni-hamburg.de\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 5\n",
    "**Description:** Implement adaboost <br>\n",
    "**Deadline:** Saturday, 29. Mai 2021, 23:59 <br>\n",
    "**Working together:** You can work in pairs or triples but no larger teams are allowed. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; Please adhere to the honor code discussed in class. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; All members of the team must get involved in understanding and coding the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Submission: \n",
    "**Christoph Brauer, Linus Geewe, Moritz Lahann**\n",
    "\n",
    "*Also put high-level comments that should be read before looking at your code and results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Goal\n",
    "The goal of this exercise is to implement Boosting based on a very simple base classifier.\n",
    "\n",
    "You implementation should be sufficiently generic to <strong>handle an arbitrary number of dimensions</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Choose your data\n",
    "\n",
    "**Task 1** (10%): Choose and load your data.\n",
    "\n",
    "For this exercise, you can choose between multiple datasets.\n",
    "Choose either `dataCircle.txt`, which contains 2-dimensional points with the corresponding class label. The first 40 rows contain positive examples with the label 1 and the other 62 rows have negative examples with the label -1.\n",
    "Alternatively, you may find it more interesting to use the features you extracted in the previous tasks from faces (you may use your own features, or use the feature computations from the sample solution). \n",
    "It will be more interesting to use the full set of faces, not just the small subset. \n",
    "(However, you may want to limit visualizations to 2 dimensions.)\n",
    "\n",
    "Your implementation of the classifier shall not be limited to a fixed number of feature dimensions (e.g., 2 as in `dataCircle.txt`, or 6 as in your previous work on face detection) but should work with any number of features. However, there's no need to implement the data loading / feature computation for both data sets.\n",
    "\n",
    "Set aside some randomly selected data as evaluation set. Alternatively you may want to re-use k-fold crossvalidation as implemented before (or from a sample solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(102, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "data = np.loadtxt('dataCircle.txt')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Implement a simple classifier, implement weighted sample evaluation, and ensure that classifiers are (at least) \"weak\"\n",
    "\n",
    "**Task 2** (30%):\n",
    "\n",
    "Start by implementing a function that returns a simple <strong>decision stump</strong> classifier for training data and given sample weights (many of these weak classifiers will later be combined).\n",
    "Standard AdaBoost searches for the best classifier in each step, by evaluating all possible classifiers in their performance on the weighted data and then choosing the one with the lowest weighted\n",
    "error. The classifiers that are considered are the boundaries between two points that change classes.\n",
    "\n",
    "In this task, you may also use a simple <strong>random decision boundary classifier </strong> that choses a random decision boundary in a randomly selected dimension of your data. (Full credit only if you implement the full AdaBoost approach to classifier selection.)\n",
    "\n",
    "Implement an evaluation function that tests the quality of a classifier on a set of data using <strong>weighted accuracy</strong>.\n",
    "\n",
    "A classifier with <50% accuracy is not a <strong>weak</strong> classifier. Remind yourself how you can build a weak binary classifier based on one with accuracy below 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self, data, weights):\n",
    "        self.train(data, weights)\n",
    "        self.threshold = 0\n",
    "        self.feature_index = 0\n",
    "\n",
    "    def train(self, data, weights):\n",
    "        # choose a random feature from our data (data is in the shape of [Feature Feature ... Class])\n",
    "        self.feature_index = random.randint(0, data.shape[1] - 2)\n",
    "\n",
    "        # set a random decision boundary (within our data's possible range)\n",
    "        print(np.min(data))\n",
    "        print(np.max(data))\n",
    "        self.threshold = random.uniform(np.min(data), np.max(data))\n",
    "        print(self.threshold)\n",
    "        \n",
    "\n",
    "    def predict(self, sample):\n",
    "        return 1 if sample[self.feature_index] > self.threshold else -1\n",
    "\n",
    "def eval_classifier(data, weights, classifier):\n",
    "    true_classes = data[:, -1]\n",
    "    predicted_classes = [classifier.predict(sample) for sample in data]\n",
    "    accuracy = 0\n",
    "    for index in range(len(weights)):\n",
    "        if true_classes[index] == predicted_classes[index]:\n",
    "            accuracy += weights[index]\n",
    "\n",
    "    # maybe:\n",
    "    # np.sum(np.equal(true_classes, predicted_classes) * weights)\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### AdaBoost\n",
    "\n",
    "**Task 3** (30%): Use the previous function that creates weak classifiers to implement AdaBoost: \n",
    "\n",
    "Initialize weights, select weak classifier, compute alpha, reweigh samples, iterate.\n",
    "\n",
    "If you use random decision boundaries for your weak classifiers, the classifier added in each iteration isn't optimal and you may need a high number of iterations until your algorithm performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self):\n",
    "        self.classifiers = []\n",
    "        self.classifier_weights = []\n",
    "        self.weights = []\n",
    "\n",
    "\n",
    "    def train(self, data, epochs):\n",
    "        self.weights = [1 / len(data)] * len(data)\n",
    "        for ep in range(epochs):\n",
    "            classifier = DecisionStump(data, self.weights)\n",
    "            acc = eval_classifier(data, self.weights, classifier)\n",
    "            print(acc)\n",
    "\n",
    "            error = self.error_candidate(data, classifier)\n",
    "            alpha = self.alpha(error)\n",
    "\n",
    "            self.classifier_weights.append(alpha)\n",
    "            self.classifiers.append(classifier)\n",
    "\n",
    "            best_error, best_classifier = self.select_classifier(data)\n",
    "            alpha = self.alpha(best_error)\n",
    "\n",
    "            self.weights = self.new_weights(alpha)\n",
    "\n",
    "\n",
    "    def select_classifier(self, data):\n",
    "        errors = [self.error_candidate(data, classifier) for classifier in self.classifiers]\n",
    "        min_error = min(errors)\n",
    "        min_classifier_index = errors.index(min_error)\n",
    "        return min_error, self.classifiers[min_classifier_index]\n",
    "            \n",
    "    \n",
    "    def error_candidate(self, data, classifier):\n",
    "        sum = 0\n",
    "        for sample, weight in zip(data, self.weights):\n",
    "            predicted = classifier.predict(sample)\n",
    "            if predicted != sample[2]:\n",
    "                sum += weight\n",
    "        return sum\n",
    "\n",
    "\n",
    "    def alpha(self, error):\n",
    "        return 0.5 * math.log((1 - error) / error)\n",
    "\n",
    "\n",
    "    def new_weights(self, alpha):\n",
    "        new_weights = np.array([weight * math.exp(alpha) for weight in self.weights])\n",
    "        return new_weights / np.sum(new_weights)\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        y_pred = []\n",
    "        for sample in data:\n",
    "            predictions = np.array([classifier.predict(sample) for classifier in self.classifiers])\n",
    "            y_pred.append(math.copysign(1.0, sum(predictions * np.array(self.classifier_weights))))\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-9.97164\n",
      "9.99208\n",
      "5.37714107980991\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "1.876606500703824\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "-7.245778852575333\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "7.292822839808874\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "5.30836563636184\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "9.007733089476023\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "5.470052319303763\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "9.295216874404446\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "4.6157431917861675\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "6.8679004695911985\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "-3.0240634714595176\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "4.27380850084201\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "2.083130159077209\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "7.108685160516984\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "0.07890278000513362\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "0.03991474204770995\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "-1.465585681431115\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "-4.512479791701816\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "4.364622969185614\n",
      "0.5000000000000002\n",
      "-9.97164\n",
      "9.99208\n",
      "1.5778011947794113\n",
      "0.5000000000000002\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "def accuracy(true, pred):\n",
    "    sum = 0\n",
    "    for index in range(len(true)):\n",
    "        sum += true[index] == pred[index]\n",
    "    return sum / len(true)\n",
    "\n",
    "data = np.loadtxt('dataCircle.txt')\n",
    "np.random.shuffle(data)\n",
    "split_index = round(data.shape[0] * 0.8)\n",
    "train = data[:split_index]\n",
    "test = data[split_index:]\n",
    "\n",
    "ensemble = AdaBoost()\n",
    "ensemble.train(train, 20)\n",
    "pred = ensemble.predict(test)\n",
    "acc = accuracy(test[:, -1], pred)\n",
    "print(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluate your classifier and plot its inner workings\n",
    "\n",
    "**Task 4** (15%):\n",
    "\n",
    "Evaluate your training set error, as well as evaluation set errors over the iterations. For this it may be convenient if your final weighted ensemble classifier that is trained via AdaBoost can be restricted to use only the first _m_ classifiers (and alphas) afterwards.\n",
    "\n",
    "#### Plot decision boundary/areas\n",
    "\n",
    "**Task 5** (15%):\n",
    "\n",
    "(If you use face data: pick two relevant feature dimensions only for this subtask and use only a small subset of the training data if runtime becomes an issue.)\n",
    "\n",
    "Plot the decisions taken by your classifier in one of the following ways (or both): \n",
    " * plot the first, second and third decision boundaries chosen by AdaBoost in a succession of plots. Also, plot the training samples in a size that is proportional to their weight after the first, second and third decision those same plots. Explain how the weight changes influence the next iteration's behaviour.\n",
    " * decide on the resolution of your image matrix (e.g., use a resolution of 100 samples over the $x_1$ and the $x_2$ range of your data), record the decisions of your classifier for all $x_1$/$x_2$ coordinates and color the image's pixels according to the decision. Plot the image. Add the training data as colored points to the plot as well. You may consult the corresponding code in the sample solution for Softmax classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Report Submission\n",
    "\n",
    "Prepare a report of your solution as a commented Jupyter notebook (using markdown for your results and comments); include figures and results.\n",
    "If you must, you can also upload a PDF document with the report annexed with your Python code.\n",
    "\n",
    "Upload your report file to the Machine Learning Moodle Course page. Please make sure that your submission team corresponds to the team's Moodle group that you're in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0936909ed86daa26a0ac8b1eb938f5d4b7bf4e1ec07caee18ace3af08811b1082",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}