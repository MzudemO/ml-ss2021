{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Machine Learning SoSe21 Practice Class\n",
    "\n",
    "Dr. Timo Baumann, Dr. Özge Alaçam, Björn Sygo <br>\n",
    "Email: baumann@informatik.uni-hamburg.de, alacam@informatik.uni-hamburg.de, 6sygo@informatik.uni-hamburg.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 7\n",
    "**Description:** Implement and simulate a multi-armed bandit  <br>\n",
    "**Deadline:** Saturday, 03. July 2021, 23:59 <br>\n",
    "**Working together:** You can work in pairs or triples but no larger teams are allowed. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; Please adhere to the honor code discussed in class. <br>\n",
    "&emsp;&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; All members of the team must get involved in understanding and coding the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Submission: \n",
    "**Put your names here**\n",
    "\n",
    "*Also put high-level comments that should be read before looking at your code and results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Goal\n",
    "In this exercise, you will implement basic RL techniques for the multi-armed bandit problem. Unlike in the previous tasks, this one does not require _data_ but a specification of a _World_, an _Agent_ and a simulation environment that supervises the performance of the agent in the world and measures its succcess. In other words: you first need to design your experimentation environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Design the interfaces of your simulation environment\n",
    "You want to separate the agent and the world and specify the interfaces of the agent and the world that it interacts with. You will then supervise the interaction in your simulation environment.\n",
    "\n",
    "**Task 1** (15%):\n",
    "To this end, define the interfaces for:\n",
    " * a k-armed bandit world (each arm's reward is normally distributed with $\\sigma=1$ and each mean is chosen normally with $\\mu=1$ and $\\sigma=1$), \n",
    " * a k-armed bandit agent that plays in the world and is able to be informed about the rewards for its actions.\n",
    "Note that it may be easier in Python to specify classes (with minimal behaviour) rather than interfaces. For example, your minimal agent could simply pull one of the arms at random.\n",
    "\n",
    "**Task 2** (10%): Define a simulation environment implementation that orchestrates the interplay between the agent and the world for a given number of action-reward rounds while keeping track of the relevant performance metrics that later need to be visualized (see below).\n",
    "\n",
    "Results of individual runs of the simulation will be very noisy. Therefore, define a way to repeatedly play the simulation (say, 2000 times) and average the performance metrics across these episodes.\n",
    "\n",
    "Perform your experiments with a $k=10$-armed bandit and episodes of $N=1000$ action-reward rounds, but make sure that your implementation also works for different $k$ and $N$.\n",
    "\n",
    "Note: analyse the requirements of the simulation environment wrt. the tasks specified below to ensure that you do all that is necessary (and not too much else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BaseWorld:\n",
    "    def __init__(self, k):\n",
    "        self.arms = [random.gauss(mu=1, sigma=1) for _ in range(k)]\n",
    "    \n",
    "    def get_reward(self, arm):\n",
    "        return random.gauss(mu=self.arms[arm], sigma=1)\n",
    "\n",
    "    def nr_arms(self):\n",
    "        return len(self.arms)\n",
    "\n",
    "    def after_iteration(self):\n",
    "        pass\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "        self.possible_actions = world.nr_arms()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.weights = [1 / self.possible_actions] * self.possible_actions\n",
    "\n",
    "    def perform_action(self):\n",
    "        action = random.choice(range(self.possible_actions), weights=self.weights, k=1)\n",
    "        reward = self.world.get_reward(action)\n",
    "        self.reward = reward\n",
    "        return reward, max(self.weights)\n",
    "\n",
    "    def info(self):\n",
    "        return \"Random Choice Agent\"\n",
    "\n",
    "class Simulation:\n",
    "    def __init__(self, agent, world, k, n):\n",
    "        self.world = world(k)\n",
    "        self.agent = agent(self.world)\n",
    "        self.epochs = n\n",
    "\n",
    "    def run_sim(self):\n",
    "        self.rewards = []\n",
    "        self.proportions = []\n",
    "        self.agent.initialize()\n",
    "        for e in range(self.epochs):\n",
    "            reward, best_action_proportion = self.agent.perform_action()\n",
    "            self.rewards.append(reward)\n",
    "            self.proportions.append(best_action_proportion)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualization\n",
    "**Task 3** (10%): Plot the performance of an agent over time (i.e., the action-reward rounds) in terms of reward achieved and proportion of best action chosen. Test the visualization by running your trivial agent (which just randomly pulls any trigger): you should not notice any improvements over time.\n",
    "\n",
    "Note: it will be convenient if your visualization functionality can plot results from multiple different agents/agent runs to simplify comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lol\nlol\nxD\nxD\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implement Reinforcement Learning\n",
    "\n",
    "**Task 4** (35%): Implement k-armed bandit agents for your environment. In particular, implement an $\\varepsilon$-greedy agent with fixed $\\varepsilon$ (experiment with $\\varepsilon \\in {0, 0.01, 0.1, 1}$) and one with UCB action selection. Your basic agents may make use of the length of the episode $N$.\n",
    "\n",
    "Furthermore, build some of the variations of $\\varepsilon$-greedy agents: optimistic initialization and allowing for arbitrarily long episodes (i.e., not using incremental Q computation).\n",
    "\n",
    "In your implementation, try to build an abstraction hierarchy that avoids re-writing code shared by multiple agent types.\n",
    "\n",
    "**Task 5** (10%): Simulate learning over 2000 episodes and visualize the results. Discuss your findings for the different settings and the various learning strategies that you have implemented.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reward drift\n",
    "**Task 6** (20%): Implement a world in which rewards of each action change gradually. Implement an agent that is suitable for such a world and compare its behaviour against the standard $\\varepsilon$-greedy agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hint\n",
    "\n",
    "More detailed information about this topic you can find in Sutton&Barto Ch. 2, which is uploaded on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Report Submission\n",
    "\n",
    "Prepare a report of your solution as a commented Jupyter notebook (using markdown for your results and comments); include figures and results.\n",
    "If you must, you can also upload a PDF document with the report annexed with your Python code.\n",
    "\n",
    "Upload your report file to the Machine Learning Moodle Course page. Please make sure that your submission team corresponds to the team's Moodle group that you're in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('ias': conda)"
  },
  "interpreter": {
   "hash": "2385fbc91a5db8d5090cc82905767e96a94c7455855faa84762d00a0cbc4e245"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}